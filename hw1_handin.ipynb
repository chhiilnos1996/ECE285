{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 285 – MLIP – Assignment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools\n",
    "xtrain, ltrain = MNISTtools.load(dataset=\"training\", path=\"/datasets/MNIST/\")\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)\n",
    "print('done 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.What are the shapes of both variables? \n",
    "<br>ans : (784, 60000) and (60000,1)\n",
    "<br>2.What is the size of the training dataset? \n",
    "<br>ans : 60000\n",
    "<br>3.What is the feature dimension? \n",
    "<br>ans : 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "done 2\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "w, h = 28, 28\n",
    "img_42 = xtrain[:,42]\n",
    "img_42 = np.reshape(img_42, (28, 28))\n",
    "img = Image.fromarray(img_42, 'L')\n",
    "plt.imshow(img, cmap = \"gray\")\n",
    "print(ltrain[42])\n",
    "print('done 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check that its content corresponds to its label: checked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "done 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "minElement = np.amin(xtrain)\n",
    "maxElement = np.amax(xtrain)\n",
    "print(minElement)\n",
    "print(maxElement)\n",
    "print(type(xtrain))\n",
    "print('done 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.What is the range of xtrain (minimum and maximum values)? \n",
    "<br>ans : 0,255\n",
    "<br>2.What is the type of xtrain? \n",
    "<br>ans : <class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 4\n"
     ]
    }
   ],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    y = x\n",
    "    y.astype(np.float32)\n",
    "    y = y*2/255 -1\n",
    "    return y\n",
    "xtrain_normalized = normalize_MNIST_images(xtrain)\n",
    "print('done 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "7\n",
      "done 5\n"
     ]
    }
   ],
   "source": [
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d\n",
    "dtrain = label2onehot(ltrain)\n",
    "print(dtrain[:,42])\n",
    "print(ltrain[42])\n",
    "print('done 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the one-hot code dtrain[:,42]\n",
    "corresponds to ltrain[42] : checked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 6\n"
     ]
    }
   ],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = np.argmax(d,axis=0)\n",
    "    return lbl\n",
    "print('done 6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 7\n"
     ]
    }
   ],
   "source": [
    "def softmax(a):\n",
    "    a.astype(np.float32)\n",
    "    b = np.amax(a, axis=0)\n",
    "    b = np.reshape(b,(1,b.shape[0]))\n",
    "    b = np.tile(b,(a.shape[0],1))\n",
    "    c = a - b\n",
    "    exp_a = np.exp(c)\n",
    "    d = np.sum(exp_a,axis=0)\n",
    "    d = np.reshape(d,(1,d.shape[0]))\n",
    "    d = np.tile(d,(exp_a.shape[0],1))   \n",
    "    y = np.divide(exp_a,d)\n",
    "    return y\n",
    "print('done 7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $g(a)_i = \\frac{exp(a_i)}{\\sum_{j=1}^{N} exp(a_j)}$ ... (1)\n",
    "<br>and $\\frac{d}{dx} \\frac{f}{g} = \\frac{f'g-fg'}{g^2}$ ... (2)\n",
    "<br>Plug (1) into (2) we obtain \n",
    "$\\frac{\\partial g(a)_i}{\\partial a_i} = $ \n",
    "$\\frac{exp(a_i)\\times \\sum_{j=1}^{N} exp(a_j)-exp(a_i)\\times exp(a_i)}{(\\sum_{j=1}^{N} exp(a_j))^2} = $\n",
    "$\\frac{exp(a_i)\\times(\\sum_{j=1}^{N} exp(a_j)-exp(a_i))}{(\\sum_{j=1}^{N} exp(a_j))^2} $\n",
    "<br>Which equals to \n",
    "$\\frac{exp(a_i)}{\\sum_{j=1}^{N} exp(a_j)}$\n",
    "$\\times \\frac{(\\sum_{j=1}^{N} exp(a_j)-exp(a_i))}{\\sum_{j=1}^{N} exp(a_j)} = $\n",
    "$g(a)_i\\times (1-g(a)_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $g(a)_i = \\frac{exp(a_i)}{\\sum_{n=1}^{N} exp(a_n)}$ ... (1)\n",
    "<br>and $\\frac{d}{dx} \\frac{f}{g} = \\frac{f'g-fg'}{g^2}$ ... (2)\n",
    "<br>Plug (1) into (2) we obtain \n",
    "$\\frac{\\partial g(a)_i}{\\partial a_j} = $ \n",
    "$\\frac{0-exp(a_i)\\times exp(a_j)}{(\\sum_{n=1}^{N} exp(a_n))^2} = $\n",
    "$-\\frac{exp(a_i)}{\\sum_{n=1}^{N} exp(a_n)}\\times \\frac{exp(a_j)}{\\sum_{n=1}^{N} exp(a_n)}$\n",
    "<br>Which equals to \n",
    "$-g(a)_i\\times g(a)_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta = [\\frac{\\partial g(a)}{\\partial a}]^T \\times e$ ...(1)\n",
    "<br>Where $[\\frac{\\partial g(a)}{\\partial a}]_{ii} = g(a)_i(1-g(a)_i)$ and $[\\frac{\\partial g(a)}{\\partial a}]_{ij} = -g(a)_ig(a)_j$ if $i\\ne j$ \n",
    "<br>So $[\\frac{\\partial g(a)}{\\partial a}]_{ij} =[\\frac{\\partial g(a)}{\\partial a}]_{ji}$  and thus the Jacobian of softmax is symmetric.\n",
    "<br>Therefore $[\\frac{\\partial g(a)}{\\partial a}]^T = [\\frac{\\partial g(a)}{\\partial a}] = A-B$ \n",
    "<br>Where $A_{ii} = g(a)_i$ and $A_{ij} = 0$ if $i\\ne j$, \n",
    "<br>and $B_{ij} = g(a)_ig(a)_j$\n",
    "<br>Plugging into (1) we have $\\delta = (A-B)\\times e = A\\times e-B\\times e$ \n",
    "<br>So it is obvious that $A\\times e =  g(a)\\bigotimes e$\n",
    "<br>and $B\\times e$ is a column vector with the element of row i being $(\\sum_{j=1}^{N} g(a)_je_j)g(a)_i$, and this is exactly $\\langle g(a),{e} \\rangle g(a)$\n",
    "<br>So eventually we have $\\delta = g(a)\\bigotimes e - \\langle g(a),{e} \\rangle g(a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 10\n"
     ]
    }
   ],
   "source": [
    "def softmaxp(a, e):\n",
    "    delta = np.zeros((a.shape[0],a.shape[1]))\n",
    "    c = softmax(a)\n",
    "    for i in range(a.shape[1]):\n",
    "        delta[:,i] = np.multiply(c[:,i],e[:,i])\n",
    "    b = np.zeros((a.shape[0],a.shape[1]))\n",
    "    for i in range(a.shape[1]): \n",
    "        b[:,i] = np.matmul(np.transpose(c[:,i]),e[:,i])*c[:,i]\n",
    "    delta = delta - b\n",
    "    return delta\n",
    "print('done 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.039340807622801e-07 should be smaller than 1e-6\n",
      "done 11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "eps = 1e-6 # finite difference step\n",
    "a = np.random.randn(10, 200) # random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "## treat a, e as 200 column vectors (i.e. batch size = 200)\n",
    "diff = softmaxp(a, e)\n",
    "diff_approx = (softmax(a+eps*e)-softmax(a))/eps\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')\n",
    "print('done 11')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.82515827998792e-07 is smaller than 1e-6 !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9586013049732394e-11 should be smaller than 1e-6\n",
      "done 12\n"
     ]
    }
   ],
   "source": [
    "def relu(a):\n",
    "    zeros = np.zeros(a.shape)\n",
    "    relu = np.maximum(a,zeros)\n",
    "    return relu\n",
    "def relup(a, e):\n",
    "    relu_p = np.zeros(e.shape)\n",
    "    for i in range (e.shape[1]): \n",
    "        j_relu = np.zeros((a.shape[0],a.shape[0]))\n",
    "        for j in range(a.shape[0]):\n",
    "            if a[j,i]>0:\n",
    "                j_relu[j][j] = 1\n",
    "        relu_p[:,i] = np.matmul(j_relu,e[:,i])\n",
    "    return relu_p\n",
    "eps = 1e-6 # finite difference step\n",
    "a = np.random.randn(10, 200) # random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "diff = relup(a, e)\n",
    "diff_approx = (relu(a+eps*e)-relu(a))/eps\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')\n",
    "print('done 12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jacobian matrix of relu function = J\n",
    "$J_{ii} = 1$ if $a_i>0$ and $J_{ii} = 0$ if $a_i<=0$ \n",
    "<br>(not differentialble at 0 lol)\n",
    "<br> and $J_{ij} = 0$ if $i\\ne j$\n",
    "<br> 3.9505616765734874e-11 is smaller than 1e-6 !!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 13\n"
     ]
    }
   ],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "print('done 13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of dividing np.sqrt((Ni+1.)/2. and np.sqrt((Nh+1.)) are called He and Xavier initializations！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 14\n"
     ]
    }
   ],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    return y\n",
    "yinit = forwardprop_shallow(xtrain_normalized, netinit)\n",
    "print('done 14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape =  (10, 60000)\n",
      "d.shape =  (10, 60000)\n",
      "loss =  [[6.49717818e-05]]\n",
      "loss =  [[0.09196369]]\n",
      "loss =  [[0.18338713]]\n",
      "loss =  [[0.27530654]]\n",
      "loss =  [[0.36836786]]\n",
      "loss =  [[0.45980052]]\n",
      "loss =  [[0.55210518]]\n",
      "loss =  [[0.64359702]]\n",
      "loss =  [[0.73596353]]\n",
      "loss =  [[0.82734432]]\n",
      "loss =  [[0.92058244]]\n",
      "loss =  [[1.01236992]]\n",
      "loss =  [[1.10496438]]\n",
      "loss =  [[1.19713488]]\n",
      "loss =  [[1.28958501]]\n",
      "loss =  [[1.38277178]]\n",
      "loss =  [[1.47504083]]\n",
      "loss =  [[1.56788728]]\n",
      "loss =  [[1.65937546]]\n",
      "loss =  [[1.75041703]]\n",
      "loss =  [[1.8437843]]\n",
      "loss =  [[1.93791809]]\n",
      "loss =  [[2.02937091]]\n",
      "loss =  [[2.12125126]]\n",
      "loss =  [[2.21414371]]\n",
      "loss =  [[2.30596574]]\n",
      "loss =  [[2.39775636]]\n",
      "loss =  [[2.4907405]]\n",
      "loss =  [[2.58220992]]\n",
      "loss =  [[2.67458606]]\n",
      "[[2.76638147]] should be around .26\n",
      "done 15\n"
     ]
    }
   ],
   "source": [
    "def eval_loss(y, d):\n",
    "    loss = np.zeros((1,1))\n",
    "    loss.astype(float)\n",
    "    print('y.shape = ',y.shape)\n",
    "    print('d.shape = ',d.shape)\n",
    "    for i in range(d.shape[1]):\n",
    "        p = np.dot(d[:,i],np.log(y[:,i]))\n",
    "        p = p/d.shape[1]\n",
    "        p = p*(-1)\n",
    "        if np.isnan(p):\n",
    "            loss = loss\n",
    "        elif np.isinf(p):\n",
    "            loss = loss\n",
    "        else :\n",
    "            loss = loss + p\n",
    "        if i%2000==0:\n",
    "            print('loss = ',loss)\n",
    "    return loss\n",
    "print(eval_loss(yinit, dtrain), 'should be around .26')\n",
    "print('done 15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape =  (10, 60000)\n",
      "predict.shape =  (60000,)\n",
      "lbl.shape =  (60000,)\n",
      "0.9353333333333333\n",
      "done 16\n"
     ]
    }
   ],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    predict = onehot2label(y)\n",
    "    print('y.shape = ',y.shape)\n",
    "    print('predict.shape = ',predict.shape)\n",
    "    print('lbl.shape = ',lbl.shape)\n",
    "    perfs = np.sum(predict == lbl)/predict.shape[0]\n",
    "    perfs = 1-perfs\n",
    "    return perfs\n",
    "print(eval_perfs(yinit, ltrain))\n",
    "print('done 16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 17\n"
     ]
    }
   ],
   "source": [
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    # x has shape (784,6000)\n",
    "    # d has shape (10,6000)\n",
    "    lbl = label2onehot(d)\n",
    "    print('x.shape[1] = ', x.shape[1])\n",
    "    for s in range (x.shape[1]): # update for each sample \n",
    "        if s%2000==0:\n",
    "            print('s = ', s)\n",
    "        xs = np.reshape(x[:,s],(x[:,s].shape[0],1))\n",
    "        a1 = W1.dot(xs) + b1\n",
    "        h1 = relu(a1)\n",
    "        a2 = W2.dot(h1) + b2\n",
    "        y = softmax(a2)\n",
    "        label = np.reshape(lbl[:,s],(lbl[:,s].shape[0],1))\n",
    "        e = -np.divide(label,y)\n",
    "        # Backward phase\n",
    "        delta2 = softmaxp(a2,e) \n",
    "        delta1 = relup(a1,h1) * np.transpose(W2).dot(delta2)\n",
    "        # gradient update\n",
    "        W2 = W2 - gamma * delta2.dot(h1.T)\n",
    "        W1 = W1 - gamma * delta1.dot(xs.T)  \n",
    "        b2 = b2 - gamma * delta2.sum( axis =1 , keepdims = True )\n",
    "        b1 = b1 - gamma * delta1.sum( axis =1 , keepdims = True )\n",
    "    return W1, b1, W2, b2\n",
    "print('done 17')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccc\n",
      "backprop_shallow\n",
      "t =  0\n",
      "x.shape[1] =  60000\n",
      "s =  0\n",
      "s =  2000\n",
      "s =  4000\n",
      "s =  6000\n",
      "s =  8000\n",
      "s =  10000\n",
      "s =  12000\n",
      "s =  14000\n",
      "s =  16000\n",
      "s =  18000\n",
      "s =  20000\n",
      "s =  22000\n",
      "s =  24000\n",
      "s =  26000\n",
      "s =  28000\n",
      "s =  30000\n",
      "s =  32000\n",
      "s =  34000\n",
      "s =  36000\n",
      "s =  38000\n",
      "s =  40000\n",
      "s =  42000\n",
      "s =  44000\n",
      "s =  46000\n",
      "s =  48000\n",
      "s =  50000\n",
      "s =  52000\n",
      "s =  54000\n",
      "s =  56000\n",
      "s =  58000\n",
      "y.shape =  (10, 60000)\n",
      "d.shape =  (10, 60000)\n",
      "loss =  [[4.91230636e-05]]\n",
      "loss =  [[0.0773207]]\n",
      "loss =  [[0.15479363]]\n",
      "loss =  [[0.23245027]]\n",
      "loss =  [[0.31059351]]\n",
      "loss =  [[0.3884185]]\n",
      "loss =  [[0.46595468]]\n",
      "loss =  [[0.54371831]]\n",
      "loss =  [[0.62150855]]\n",
      "loss =  [[0.69899683]]\n",
      "loss =  [[0.77719023]]\n",
      "loss =  [[0.85496509]]\n",
      "loss =  [[0.93309139]]\n",
      "loss =  [[1.01100073]]\n",
      "loss =  [[1.08918733]]\n",
      "loss =  [[1.16737347]]\n",
      "loss =  [[1.24501824]]\n",
      "loss =  [[1.32340502]]\n",
      "loss =  [[1.40117562]]\n",
      "loss =  [[1.47901637]]\n",
      "loss =  [[1.5570276]]\n",
      "loss =  [[1.63544791]]\n",
      "loss =  [[1.7130952]]\n",
      "loss =  [[1.79076864]]\n",
      "loss =  [[1.8689429]]\n",
      "loss =  [[1.94680766]]\n",
      "loss =  [[2.0248108]]\n",
      "loss =  [[2.10299247]]\n",
      "loss =  [[2.18117622]]\n",
      "loss =  [[2.25880408]]\n",
      "eval_loss(y,d) [[2.33644245]]\n",
      "y.shape =  (10, 60000)\n",
      "predict.shape =  (60000,)\n",
      "lbl.shape =  (60000,)\n",
      "eval_perfs(y,lbl) 0.91705\n",
      "t =  1\n",
      "x.shape[1] =  60000\n",
      "s =  0\n",
      "s =  2000\n",
      "s =  4000\n",
      "s =  6000\n",
      "s =  8000\n",
      "s =  10000\n",
      "s =  12000\n",
      "s =  14000\n",
      "s =  16000\n",
      "s =  18000\n",
      "s =  20000\n",
      "s =  22000\n",
      "s =  24000\n",
      "s =  26000\n",
      "s =  28000\n",
      "s =  30000\n",
      "s =  32000\n",
      "s =  34000\n",
      "s =  36000\n",
      "s =  38000\n",
      "s =  40000\n",
      "s =  42000\n",
      "s =  44000\n",
      "s =  46000\n",
      "s =  48000\n",
      "s =  50000\n",
      "s =  52000\n",
      "s =  54000\n",
      "s =  56000\n",
      "s =  58000\n",
      "y.shape =  (10, 60000)\n",
      "d.shape =  (10, 60000)\n",
      "loss =  [[4.76209732e-05]]\n",
      "loss =  [[0.07534363]]\n",
      "loss =  [[0.15082028]]\n",
      "loss =  [[0.22626105]]\n",
      "loss =  [[0.30212234]]\n",
      "loss =  [[0.37780479]]\n",
      "loss =  [[0.45324874]]\n",
      "loss =  [[0.52898556]]\n",
      "loss =  [[0.60460709]]\n",
      "loss =  [[0.68012285]]\n",
      "loss =  [[0.7559584]]\n",
      "loss =  [[0.83159071]]\n",
      "loss =  [[0.9076024]]\n",
      "loss =  [[0.98343023]]\n",
      "loss =  [[1.05944516]]\n",
      "loss =  [[1.13544468]]\n",
      "loss =  [[1.21111771]]\n",
      "loss =  [[1.28738094]]\n",
      "loss =  [[1.36312492]]\n",
      "loss =  [[1.43906136]]\n",
      "loss =  [[1.51486595]]\n",
      "loss =  [[1.59096519]]\n",
      "loss =  [[1.66664878]]\n",
      "loss =  [[1.74224453]]\n",
      "loss =  [[1.81813981]]\n",
      "loss =  [[1.89391312]]\n",
      "loss =  [[1.96989214]]\n",
      "loss =  [[2.04585033]]\n",
      "loss =  [[2.12201227]]\n",
      "loss =  [[2.19751635]]\n",
      "eval_loss(y,d) [[2.27308098]]\n",
      "y.shape =  (10, 60000)\n",
      "predict.shape =  (60000,)\n",
      "lbl.shape =  (60000,)\n",
      "eval_perfs(y,lbl) 0.8422833333333333\n",
      "t =  2\n",
      "x.shape[1] =  60000\n",
      "s =  0\n",
      "s =  2000\n",
      "s =  4000\n",
      "s =  6000\n",
      "s =  8000\n",
      "s =  10000\n",
      "s =  12000\n",
      "s =  14000\n",
      "s =  16000\n",
      "s =  18000\n",
      "s =  20000\n",
      "s =  22000\n",
      "s =  24000\n",
      "s =  26000\n",
      "s =  28000\n",
      "s =  30000\n",
      "s =  32000\n",
      "s =  34000\n",
      "s =  36000\n",
      "s =  38000\n",
      "s =  40000\n",
      "s =  42000\n",
      "s =  44000\n",
      "s =  46000\n",
      "s =  48000\n",
      "s =  50000\n",
      "s =  52000\n",
      "s =  54000\n",
      "s =  56000\n",
      "s =  58000\n",
      "y.shape =  (10, 60000)\n",
      "d.shape =  (10, 60000)\n",
      "loss =  [[4.72616847e-05]]\n",
      "loss =  [[0.07434335]]\n",
      "loss =  [[0.14875595]]\n",
      "loss =  [[0.22305297]]\n",
      "loss =  [[0.29778746]]\n",
      "loss =  [[0.37233792]]\n",
      "loss =  [[0.44671796]]\n",
      "loss =  [[0.52141002]]\n",
      "loss =  [[0.59595524]]\n",
      "loss =  [[0.67046982]]\n",
      "loss =  [[0.74510639]]\n",
      "loss =  [[0.81959269]]\n",
      "loss =  [[0.89452006]]\n",
      "loss =  [[0.96928013]]\n",
      "loss =  [[1.04419193]]\n",
      "loss =  [[1.11907647]]\n",
      "loss =  [[1.1938133]]\n",
      "loss =  [[1.26899663]]\n",
      "loss =  [[1.34368615]]\n",
      "loss =  [[1.41860778]]\n",
      "loss =  [[1.49332602]]\n",
      "loss =  [[1.56829632]]\n",
      "loss =  [[1.64297224]]\n",
      "loss =  [[1.71754111]]\n",
      "loss =  [[1.79229386]]\n",
      "loss =  [[1.86696625]]\n",
      "loss =  [[1.9418794]]\n",
      "loss =  [[2.01668749]]\n",
      "loss =  [[2.09173017]]\n",
      "loss =  [[2.16617964]]\n",
      "eval_loss(y,d) [[2.24065345]]\n",
      "y.shape =  (10, 60000)\n",
      "predict.shape =  (60000,)\n",
      "lbl.shape =  (60000,)\n",
      "eval_perfs(y,lbl) 0.8223333333333334\n",
      "t =  3\n",
      "x.shape[1] =  60000\n",
      "s =  0\n",
      "s =  2000\n",
      "s =  4000\n",
      "s =  6000\n",
      "s =  8000\n",
      "s =  10000\n",
      "s =  12000\n",
      "s =  14000\n",
      "s =  16000\n",
      "s =  18000\n",
      "s =  20000\n",
      "s =  22000\n",
      "s =  24000\n",
      "s =  26000\n",
      "s =  28000\n",
      "s =  30000\n",
      "s =  32000\n",
      "s =  34000\n",
      "s =  36000\n",
      "s =  38000\n",
      "s =  40000\n",
      "s =  42000\n",
      "s =  44000\n",
      "s =  46000\n",
      "s =  48000\n",
      "s =  50000\n",
      "s =  52000\n",
      "s =  54000\n",
      "s =  56000\n",
      "s =  58000\n",
      "y.shape =  (10, 60000)\n",
      "d.shape =  (10, 60000)\n",
      "loss =  [[4.70134167e-05]]\n",
      "loss =  [[0.07349984]]\n",
      "loss =  [[0.14701249]]\n",
      "loss =  [[0.22038938]]\n",
      "loss =  [[0.29421956]]\n",
      "loss =  [[0.36784515]]\n",
      "loss =  [[0.44134368]]\n",
      "loss =  [[0.5151759]]\n",
      "loss =  [[0.58887905]]\n",
      "loss =  [[0.6625652]]\n",
      "loss =  [[0.7362441]]\n",
      "loss =  [[0.80979156]]\n",
      "loss =  [[0.88383235]]\n",
      "loss =  [[0.95772268]]\n",
      "loss =  [[1.03175446]]\n",
      "loss =  [[1.10573147]]\n",
      "loss =  [[1.17971524]]\n",
      "loss =  [[1.25401008]]\n",
      "loss =  [[1.32782864]]\n",
      "loss =  [[1.40188679]]\n",
      "loss =  [[1.47573603]]\n",
      "loss =  [[1.54982233]]\n",
      "loss =  [[1.62366979]]\n",
      "loss =  [[1.69740818]]\n",
      "loss =  [[1.77125354]]\n",
      "loss =  [[1.84502852]]\n",
      "loss =  [[1.91906837]]\n",
      "loss =  [[1.99294584]]\n",
      "loss =  [[2.06704533]]\n",
      "loss =  [[2.14062776]]\n",
      "eval_loss(y,d) [[2.21419102]]\n",
      "y.shape =  (10, 60000)\n",
      "predict.shape =  (60000,)\n",
      "lbl.shape =  (60000,)\n",
      "eval_perfs(y,lbl) 0.8010666666666667\n",
      "t =  4\n",
      "x.shape[1] =  60000\n",
      "s =  0\n",
      "s =  2000\n",
      "s =  4000\n",
      "s =  6000\n",
      "s =  8000\n",
      "s =  10000\n",
      "s =  12000\n",
      "s =  14000\n",
      "s =  16000\n",
      "s =  18000\n",
      "s =  20000\n",
      "s =  22000\n",
      "s =  24000\n",
      "s =  26000\n",
      "s =  28000\n",
      "s =  30000\n",
      "s =  32000\n",
      "s =  34000\n",
      "s =  36000\n",
      "s =  38000\n",
      "s =  40000\n",
      "s =  42000\n",
      "s =  44000\n",
      "s =  46000\n",
      "s =  48000\n",
      "s =  50000\n",
      "s =  52000\n",
      "s =  54000\n",
      "s =  56000\n",
      "s =  58000\n",
      "y.shape =  (10, 60000)\n",
      "d.shape =  (10, 60000)\n",
      "loss =  [[4.65675241e-05]]\n",
      "loss =  [[0.07264874]]\n",
      "loss =  [[0.14526319]]\n",
      "loss =  [[0.21774315]]\n",
      "loss =  [[0.29068929]]\n",
      "loss =  [[0.36342346]]\n",
      "loss =  [[0.43604826]]\n",
      "loss =  [[0.50904731]]\n",
      "loss =  [[0.58192867]]\n",
      "loss =  [[0.65480846]]\n",
      "loss =  [[0.72755941]]\n",
      "loss =  [[0.80019211]]\n",
      "loss =  [[0.8733714]]\n",
      "loss =  [[0.94641678]]\n",
      "loss =  [[1.01961156]]\n",
      "loss =  [[1.09270261]]\n",
      "loss =  [[1.16594355]]\n",
      "loss =  [[1.23938087]]\n",
      "loss =  [[1.31235158]]\n",
      "loss =  [[1.38553793]]\n",
      "loss =  [[1.45853875]]\n",
      "loss =  [[1.53178102]]\n",
      "loss =  [[1.60479957]]\n",
      "loss =  [[1.67772452]]\n",
      "loss =  [[1.75069602]]\n",
      "loss =  [[1.82359933]]\n",
      "loss =  [[1.89678365]]\n",
      "loss =  [[1.96975877]]\n",
      "loss =  [[2.04293748]]\n",
      "loss =  [[2.11566657]]\n",
      "eval_loss(y,d) [[2.18831516]]\n",
      "y.shape =  (10, 60000)\n",
      "predict.shape =  (60000,)\n",
      "lbl.shape =  (60000,)\n",
      "eval_perfs(y,lbl) 0.7721333333333333\n",
      "t =  5\n",
      "x.shape[1] =  60000\n",
      "s =  0\n",
      "s =  2000\n",
      "s =  4000\n",
      "s =  6000\n",
      "s =  8000\n",
      "s =  10000\n",
      "s =  12000\n",
      "s =  14000\n",
      "s =  16000\n",
      "s =  18000\n",
      "s =  20000\n",
      "s =  22000\n",
      "s =  24000\n",
      "s =  26000\n",
      "s =  28000\n",
      "s =  30000\n",
      "s =  32000\n",
      "s =  34000\n",
      "s =  36000\n",
      "s =  38000\n",
      "s =  40000\n",
      "s =  42000\n",
      "s =  44000\n",
      "s =  46000\n",
      "s =  48000\n",
      "s =  50000\n",
      "s =  52000\n",
      "s =  54000\n",
      "s =  56000\n",
      "s =  58000\n",
      "y.shape =  (10, 60000)\n",
      "d.shape =  (10, 60000)\n",
      "loss =  [[4.61132002e-05]]\n",
      "loss =  [[0.07176324]]\n",
      "loss =  [[0.143432]]\n",
      "loss =  [[0.21498274]]\n",
      "loss =  [[0.28701603]]\n",
      "loss =  [[0.35883363]]\n",
      "loss =  [[0.43054501]]\n",
      "loss =  [[0.50266919]]\n",
      "loss =  [[0.57470231]]\n",
      "loss =  [[0.64674649]]\n",
      "loss =  [[0.71853826]]\n",
      "loss =  [[0.7902346]]\n",
      "loss =  [[0.86252974]]\n",
      "loss =  [[0.93469379]]\n",
      "loss =  [[1.00702175]]\n",
      "loss =  [[1.07918846]]\n",
      "loss =  [[1.15164819]]\n",
      "loss =  [[1.22419917]]\n",
      "loss =  [[1.2962855]]\n",
      "loss =  [[1.36856063]]\n",
      "loss =  [[1.44066137]]\n",
      "loss =  [[1.51302948]]\n",
      "loss =  [[1.58517382]]\n",
      "loss =  [[1.65724619]]\n",
      "loss =  [[1.72930081]]\n",
      "loss =  [[1.80129303]]\n",
      "loss =  [[1.87357342]]\n",
      "loss =  [[1.94561657]]\n",
      "loss =  [[2.01785743]]\n",
      "loss =  [[2.08969525]]\n",
      "eval_loss(y,d) [[2.1613881]]\n",
      "y.shape =  (10, 60000)\n",
      "predict.shape =  (60000,)\n",
      "lbl.shape =  (60000,)\n",
      "eval_perfs(y,lbl) 0.7397666666666667\n",
      "t =  6\n",
      "x.shape[1] =  60000\n",
      "s =  0\n",
      "s =  2000\n",
      "s =  4000\n",
      "s =  6000\n",
      "s =  8000\n",
      "s =  10000\n",
      "s =  12000\n",
      "s =  14000\n",
      "s =  16000\n",
      "s =  18000\n",
      "s =  20000\n",
      "s =  22000\n",
      "s =  24000\n",
      "s =  26000\n",
      "s =  28000\n",
      "s =  30000\n",
      "s =  32000\n",
      "s =  34000\n",
      "s =  36000\n",
      "s =  38000\n",
      "s =  40000\n",
      "s =  42000\n",
      "s =  44000\n",
      "s =  46000\n",
      "s =  48000\n",
      "s =  50000\n",
      "s =  52000\n",
      "s =  54000\n",
      "s =  56000\n",
      "s =  58000\n",
      "y.shape =  (10, 60000)\n",
      "d.shape =  (10, 60000)\n",
      "loss =  [[4.55788783e-05]]\n",
      "loss =  [[0.07082857]]\n",
      "loss =  [[0.14150831]]\n",
      "loss =  [[0.21207511]]\n",
      "loss =  [[0.28315615]]\n",
      "loss =  [[0.35399812]]\n",
      "loss =  [[0.42473697]]\n",
      "loss =  [[0.49594172]]\n",
      "loss =  [[0.56707761]]\n",
      "loss =  [[0.63824285]]\n",
      "loss =  [[0.70902477]]\n",
      "loss =  [[0.77973495]]\n",
      "loss =  [[0.85109131]]\n",
      "loss =  [[0.92231074]]\n",
      "loss =  [[0.99371129]]\n",
      "loss =  [[1.06490042]]\n",
      "loss =  [[1.13652829]]\n",
      "loss =  [[1.20814782]]\n",
      "loss =  [[1.27930309]]\n",
      "loss =  [[1.35061229]]\n",
      "loss =  [[1.42175062]]\n",
      "loss =  [[1.49319345]]\n",
      "loss =  [[1.5644056]]\n",
      "loss =  [[1.63557508]]\n",
      "loss =  [[1.70665653]]\n",
      "loss =  [[1.77769106]]\n",
      "loss =  [[1.84901473]]\n",
      "loss =  [[1.92008248]]\n",
      "loss =  [[1.99131661]]\n",
      "loss =  [[2.06220455]]\n",
      "eval_loss(y,d) [[2.13288145]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape =  (10, 60000)\n",
      "predict.shape =  (60000,)\n",
      "lbl.shape =  (60000,)\n",
      "eval_perfs(y,lbl) 0.7060500000000001\n",
      "t =  7\n",
      "x.shape[1] =  60000\n",
      "s =  0\n",
      "s =  2000\n",
      "s =  4000\n",
      "s =  6000\n",
      "s =  8000\n",
      "s =  10000\n",
      "s =  12000\n",
      "s =  14000\n",
      "s =  16000\n",
      "s =  18000\n",
      "s =  20000\n",
      "s =  22000\n",
      "s =  24000\n",
      "s =  26000\n",
      "s =  28000\n",
      "s =  30000\n",
      "s =  32000\n",
      "s =  34000\n",
      "s =  36000\n",
      "s =  38000\n",
      "s =  40000\n",
      "s =  42000\n",
      "s =  44000\n",
      "s =  46000\n",
      "s =  48000\n",
      "s =  50000\n",
      "s =  52000\n",
      "s =  54000\n",
      "s =  56000\n",
      "s =  58000\n",
      "y.shape =  (10, 60000)\n",
      "d.shape =  (10, 60000)\n",
      "loss =  [[4.50443046e-05]]\n",
      "loss =  [[0.06983322]]\n",
      "loss =  [[0.13944556]]\n",
      "loss =  [[0.20896427]]\n",
      "loss =  [[0.27902735]]\n",
      "loss =  [[0.34882127]]\n",
      "loss =  [[0.41851346]]\n",
      "loss =  [[0.48872938]]\n",
      "loss =  [[0.55890056]]\n",
      "loss =  [[0.62912637]]\n",
      "loss =  [[0.6988321]]\n",
      "loss =  [[0.76848547]]\n",
      "loss =  [[0.83882359]]\n",
      "loss =  [[0.90902774]]\n",
      "loss =  [[0.97941943]]\n",
      "loss =  [[1.04956415]]\n",
      "loss =  [[1.12030116]]\n",
      "loss =  [[1.19092245]]\n",
      "loss =  [[1.26108364]]\n",
      "loss =  [[1.33135148]]\n",
      "loss =  [[1.4014548]]\n",
      "loss =  [[1.47190713]]\n",
      "loss =  [[1.54211972]]\n",
      "loss =  [[1.61231303]]\n",
      "loss =  [[1.68234315]]\n",
      "loss =  [[1.75234758]]\n",
      "loss =  [[1.82264521]]\n",
      "loss =  [[1.89267067]]\n",
      "loss =  [[1.96282067]]\n",
      "loss =  [[2.03268403]]\n",
      "eval_loss(y,d) [[2.10227224]]\n",
      "y.shape =  (10, 60000)\n",
      "predict.shape =  (60000,)\n",
      "lbl.shape =  (60000,)\n",
      "eval_perfs(y,lbl) 0.6725666666666666\n",
      "t =  8\n",
      "x.shape[1] =  60000\n",
      "s =  0\n",
      "s =  2000\n",
      "s =  4000\n"
     ]
    }
   ],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    print('backprop_shallow')\n",
    "    lbl = onehot2label(d)\n",
    "    y = forwardprop_shallow(x, net)\n",
    "    for t in range(T):\n",
    "        print('t = ',t)\n",
    "        net = update_shallow(x, lbl, net, gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        print('eval_loss(y,d)', eval_loss(y, d))\n",
    "        print('eval_perfs(y,lbl)', eval_perfs(y, lbl))\n",
    "    return net\n",
    "print('ccc')\n",
    "nettrain = backprop_shallow(xtrain_normalized, dtrain, netinit, 100)\n",
    "print('done 18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest, ltest = MNISTtools.load(dataset=\"testing\", path=\"/datasets/MNIST/\")\n",
    "print(xtest.shape)\n",
    "print(ltest.shape)\n",
    "xtest_normalized = normalize_MNIST_images(xtest)\n",
    "ytest = forwardprop_shallow(xtest_normalized, net)\n",
    "print(eval_perfs(ytest, ltest))\n",
    "print('done 19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.What is the size of the testing set?\n",
    "<br>ans : 10k\n",
    "<br>2.Evaluate the performance of your network on the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    N = x.shape[1]\n",
    "    NB = int((N+B-1)/B)\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        for l in range(NB):\n",
    "            minibatch_indices = shuffled_indices[B*l:min(B*(l+1), N)]\n",
    "            x_minibatch = x[:,minibatch_indices]\n",
    "            lbl_minibatch = lbl[:,minibatch_indices]\n",
    "            net = update_shallow(x_minibatch, lbl_minibatch, net, gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        print('eval_loss(y,d)', eval_loss(y, d))\n",
    "        print('eval_perfs(y,lbl)', eval_perfs(y, lbl))\n",
    "    return net\n",
    "netminibatch = backprop_minibatch_shallow(xtrain_normalized, dtrain, netinit, 5, B=100)\n",
    "print('done 20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = forwardprop_shallow(xtest_normalized, net)\n",
    "print(eval_perfs(ytest, ltest))\n",
    "print('done 21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
